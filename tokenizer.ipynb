{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt','r', encoding ='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ഉടലെഴുതുന്ന രംഗരചനകൾ - ദേശാഭിമാനി\n",
      "\n",
      "അതിവേഗം പരിണാമങ്ങളെ സ്വാംശീകരിക്കാനും കലാത്മകമായി വികസിപ്പിക്കാനും മറ്റേതു\n",
      "ഭാരതീയ നൃത്തരൂപത്തെക്കാളും കഴിവു തെളിയിച്ച ഭരതനാട്യത്തിന് രമയെപ്പോലുള്ള\n",
      "കലാകാരികൾ അനിവാര്യമാണ്. ഭരതനാട്യത്തെ രമാവൈദ്യനാഥൻ ആവശ്യപ്പെടുന്നതിനോളം\n",
      "തന്നെ, തിരിച്ചും പ്രസക്തമായ ഈ വാസ്തവമാണ് അനേകം നർത്തകികളിൽ അവരെ\n",
      "പ്രസക്തയാക്കുന്നതും. - പ്രശസ്ത തെന്നിന്ത്യൻ നർത്തകി രമാവൈദ്യനാഥന്റെ\n",
      "കലാജീവിതത്തെക്കുറിച്ച് വേറിട്ടൊരു വിലയിരുത്തൽ.\n",
      "\n",
      "മദ്രാസ് മ്യൂസിയത്തിലെ ആർട്ട് ഗാലറിയുടെ ചിത്രസമൃദ്ധിയിൽ, എ പി സന്താനരാജിന്റെ\n",
      "ഒരു പേരിടാച്ചിത്രമുണ്ട്. ജലവിതാനത്തിലേക്ക് ഊർന്നിറങ്ങിയ ഒരു പെണ്ണുടലും ആ\n",
      "ഉടലിന്റെ തന്മ യെന്നോണം അതിലും ഉഗ്രദീപ്തമായൊരു നിഴലിന്റെ ജലദൃശ്യവും. ചുറ്റും\n",
      "തീവ്രമായൊരു ഉദ്വേഗത്തോടെ പ്രകൃതിയും കാണാം. ഉടലിനെ ജലം വായിച്ചതെങ്ങനെ എന്ന്\n",
      "ഉൽക്കണ്ഠയോടെ ഉറ്റുനോക്കുകയാണ് ചുറ്റുമുള്ള ചരാചരം. ചിത്രത്തിന് പേരിടാതെപോയത്\n",
      "എന്തുകൊണ്ടോ, അറിയില്ല. പക്ഷേ, മെയ്യെഴുത്തിന്റെ കവിത കാൻവാസിൽ ഇങ്ങനെ\n",
      "പൂത്തുലയുന്നത് അത്യപൂർവമെന്ന് നോക്കിനിൽക്കുന്തോറും ബോധ്യമാവും. ഭാരതീയ\n",
      "നൃത്തവും അതിന്റെ ഭാവനയും വാസ്തവത്തിൽ ആ പേരിടാച്ചിത്രത\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the chatracters = 23332\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the chatracters =\",len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters = {'n', 'ള', 'ശ', 'ഘ', '.', 'ങ', '\"', 'ൃ', 'ല', 'v', 'ഐ', 'ക', '`', 'ഴ', '3', 'ു', 'ഏ', 'ഭ', 'ആ', 'ഒ', 's', 'ം', 'ൂ', 'പ', ' ', '9', 'െ', 'ത', 'ഞ', 'D', 'വ', 'ഖ', 'ൽ', 'ഹ', '1', 'ഉ', 'ണ', 'p', '2', 'സ', 'ി', 'l', 'ഃ', '?', 'ർ', '(', ';', 'ച', 'അ', 'ഫ', '-', 'യ', 'ൊ', 'ന', 'd', 'e', ')', 'ഛ', 'ോ', 'o', 'ൾ', 'മ', 'ഠ', 'ഇ', 'ഓ', 'ഊ', 'ഡ', 'ദ', 'b', 'ജ', 'ധ', 'g', 'ര', 'y', 'ൗ', 'ബ', 'ൻ', 'c', 'ീ', '!', 'ഗ', 'റ', 'ാ', 'ൈ', 'ഈ', 'ൺ', 'ഥ', '്', 'ഷ', 'ട', 'േ', '\\n', 'എ', 'M', 'r', \"'\", 'a', ',', 'i', 't'}\n",
      "Number of unique characters =  100\n"
     ]
    }
   ],
   "source": [
    "chars = set(list(text))\n",
    "print (\"Unique characters =\",chars)\n",
    "vocab_size = len(chars)\n",
    "print(\"Number of unique characters = \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 67, 87, 72, 82, 39, 87]\n",
      "മദ്രാസ്\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i]for i in l])\n",
    "encoded_word = encode(\"മദ്രാസ്\")\n",
    "print(encoded_word)\n",
    "decoded_word = decode(encoded_word)\n",
    "print(decoded_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Using cached torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch) (2.4)\n",
      "Collecting nvidia-nccl-cu12==2.18.1\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m722.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.1.0\n",
      "  Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 KB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/jeslin/.local/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch) (1.9)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch) (3.6.0)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed fsspec-2023.10.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.1.1 triton-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([23332])\n",
      "tensor([35, 89,  8, 26, 13, 15, 27, 15, 53, 87, 53, 24, 72, 21, 80, 72, 47, 53,\n",
      "        11, 60, 24, 50, 24, 67, 90,  2, 82, 17, 40, 61, 82, 53, 40, 91, 91, 48,\n",
      "        27, 40, 30, 90, 80, 21, 24, 23, 72, 40, 36, 82, 61,  5, 87,  5,  1, 26,\n",
      "        24, 39, 87, 30, 82, 21,  2, 78, 11, 72, 40, 11, 87, 11, 82, 53, 15, 21,\n",
      "        24, 11,  8, 82, 27, 87, 61, 11, 61, 82, 51, 40, 24, 30, 40, 11, 39, 40,\n",
      "        23, 87, 23, 40, 11, 87, 11, 82, 53, 15, 21, 24, 61, 81, 87, 81, 90, 27,\n",
      "        15, 91, 17, 82, 72, 27, 78, 51, 24, 53,  7, 27, 87, 27, 72, 22, 23, 27,\n",
      "        87, 27, 26, 11, 87, 11, 82,  1, 15, 21, 24, 11, 13, 40, 30, 15, 24, 27,\n",
      "        26,  1, 40, 51, 40, 47, 87, 47, 24, 17, 72, 27, 53, 82, 89, 87, 51, 27,\n",
      "        87, 27, 40, 53, 87, 24, 72, 61, 51, 26, 23, 87, 23, 58,  8, 15,  1, 87,\n",
      "         1, 91, 11,  8, 82, 11, 82, 72, 40, 11, 60, 24, 48, 53, 40, 30, 82, 72,\n",
      "        87, 51, 61, 82, 36, 87,  4, 24, 17, 72, 27, 53, 82, 89, 87, 51, 27, 87,\n",
      "        27, 26, 24, 72, 61, 82, 30, 83, 67, 87, 51, 53, 82, 86, 76, 24, 18, 30,\n",
      "         2, 87, 51, 23, 87, 23, 26, 89, 15, 53, 87, 53, 27, 40, 53, 58,  1, 21,\n",
      "        91, 27, 53, 87, 53, 26, 97, 24, 27, 40, 72, 40, 47, 87, 47, 15, 21, 24,\n",
      "        23, 87, 72, 39, 11, 87, 27, 61, 82, 51, 24, 84, 24, 30, 82, 39, 87, 27,\n",
      "        30, 61, 82, 36, 87, 24, 48, 53, 90, 11, 21, 24, 53, 44, 27, 87, 27, 11,\n",
      "        40, 11,  1, 40, 32, 24, 48, 30, 72, 26, 91, 23, 87, 72, 39, 11, 87, 27,\n",
      "        51, 82, 11, 87, 11, 15, 53, 87, 53, 27, 15, 21,  4, 24, 50, 24, 23, 87,\n",
      "        72,  2, 39, 87, 27, 24, 27, 26, 53, 87, 53, 40, 53, 87, 27, 87, 51, 76,\n",
      "        24, 53, 44, 27, 87, 27, 11, 40, 24, 72, 61, 82, 30, 83, 67, 87, 51, 53,\n",
      "        82, 86, 53, 87, 81, 26, 91, 11,  8, 82, 69, 78, 30, 40, 27, 27, 87, 27,\n",
      "        26, 11, 87, 11, 15, 81, 40, 47, 87, 47, 87, 24, 30, 90, 81, 40, 89, 87,\n",
      "        89, 52, 72, 15, 24, 30, 40,  8, 51, 40, 72, 15, 27, 87, 27, 32,  4, 91,\n",
      "        91, 61, 67, 87, 72, 82, 39, 87, 24, 61, 87, 51, 22, 39, 40, 51, 27, 87,\n",
      "        27, 40,  8, 26, 24, 18, 44, 89, 87, 89, 87, 24, 80, 82,  8, 81, 40, 51,\n",
      "        15, 89, 26, 24, 47, 40, 27, 87, 72, 39, 61,  7, 67, 87, 70, 40, 51, 40,\n",
      "        32, 97, 24, 92, 24, 23, 40, 24, 39, 53, 87, 27, 82, 53, 72, 82, 69, 40,\n",
      "        53, 87, 81, 26, 91, 19, 72, 15, 24, 23, 90, 72, 40, 89, 82, 47, 87, 47,\n",
      "        40, 27, 87, 72, 61, 15, 36, 87, 89, 87,  4, 24, 69,  8, 30, 40, 27, 82,\n",
      "        53, 27, 87, 27, 40,  8, 90, 11, 87, 11, 87, 24, 65, 44, 53, 87, 53, 40,\n",
      "        81,  5, 87,  5, 40, 51, 24, 19, 72, 15, 24, 23, 26, 36, 87, 36, 15, 89,\n",
      "         8, 15, 21, 24, 18, 91, 35, 89,  8, 40, 53, 87, 81, 26, 24, 27, 53, 87,\n",
      "        61, 24, 51, 26, 53, 87, 53, 58, 36, 21, 24, 48, 27, 40,  8, 15, 21, 24,\n",
      "        35, 80, 87, 72, 67, 78, 23, 87, 27, 61, 82, 51, 52, 72, 15, 24, 53, 40,\n",
      "        13,  8, 40, 53, 87, 81, 26, 24, 69,  8, 67,  7,  2, 87, 51, 30, 15, 21,\n",
      "         4, 24, 47, 15, 81, 87, 81, 15, 21, 91, 27, 78, 30, 87, 72, 61, 82, 51,\n",
      "        52, 72, 15, 24, 35, 67, 87, 30, 90, 80, 27, 87, 27, 58, 89, 26, 24, 23,\n",
      "        87, 72, 11,  7, 27, 40, 51, 15, 21, 24, 11, 82, 36, 82, 21,  4, 24, 35,\n",
      "        89,  8, 40, 53, 26, 24, 69,  8, 21, 24, 30, 82, 51, 40, 47, 87, 47, 27,\n",
      "        26,  5, 87,  5, 53, 26, 24, 92, 53, 87, 53, 87, 91, 35, 32, 11, 87, 11,\n",
      "        36, 87, 62, 51, 58, 89, 26, 24, 35, 81, 87, 81, 15, 53, 58, 11, 87, 11,\n",
      "        15, 11, 51, 82, 36, 87, 24, 47, 15, 81, 87, 81, 15, 61, 15,  1, 87,  1,\n",
      "        24, 47, 72, 82, 47, 72, 21,  4, 24, 47, 40, 27, 87, 72, 27, 87, 27, 40,\n",
      "        53, 87, 24, 23, 90, 72, 40, 89, 82, 27, 26, 23, 58, 51, 27, 87, 91, 92,\n",
      "        53, 87, 27, 15, 11, 52, 36, 87, 89, 58, 97, 24, 48, 81, 40, 51, 40,  8,\n",
      "        87,  8,  4, 24, 23, 11, 87, 88, 90, 97, 24, 61, 26, 51, 87, 51, 26, 13,\n",
      "        15, 27, 87, 27, 40, 53, 87, 81, 26, 24, 11, 30, 40, 27, 24, 11, 82, 76,\n",
      "        30, 82, 39, 40, 32, 24, 63,  5, 87,  5, 53, 26, 91, 23, 22, 27, 87, 27,\n",
      "        15,  8, 51, 15, 53, 87, 53, 27, 87, 24, 48, 27, 87, 51, 23, 22, 44, 30,\n",
      "        61, 26, 53, 87, 53, 87, 24, 53, 58, 11, 87, 11, 40, 53, 40, 32, 11, 87,\n",
      "        11, 15, 53, 87, 27, 58, 81, 15, 21, 24, 75, 58, 70, 87, 51, 61, 82, 30,\n",
      "        15, 21,  4, 24, 17, 82, 72, 27, 78, 51, 91, 53,  7, 27, 87, 27, 30, 15,\n",
      "        21, 24, 48, 27, 40, 53, 87, 81, 26, 24, 17, 82, 30, 53, 51, 15, 21, 24,\n",
      "        30, 82, 39, 87, 27, 30, 27, 87, 27, 40, 32, 24, 18, 24, 23, 90, 72, 40,\n",
      "        89, 82, 47, 87, 47, 40, 27, 87, 72, 27])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text),dtype=torch.long)\n",
    "print(data.shape)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9* len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[87, 11, 82, 72, 27, 87, 27, 40],\n",
      "        [72, 82, 33, 87, 61, 36, 53, 44],\n",
      "        [51, 21, 24, 11, 52, 36, 87, 89],\n",
      "        [39, 82, 44, 27, 87, 72, 40, 53]])\n",
      "Targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[11, 82, 72, 27, 87, 27, 40,  8],\n",
      "        [82, 33, 87, 61, 36, 53, 44, 27],\n",
      "        [21, 24, 11, 52, 36, 87, 89, 87],\n",
      "        [82, 44, 27, 87, 72, 40, 53, 87]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size,(batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size]for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size]for i in ix])\n",
    "    return x,y\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"Targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.9322, grad_fn=<NllLossBackward0>) torch.Size([32, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "    def forward(self, idx, targets):\n",
    "        logits=self.token_embedding_table(idx)\n",
    "        B,T,C = logits.shape\n",
    "        logits = logits.view(B*T,C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits,loss\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits,loss = m(xb,yb)\n",
    "print(loss,logits.shape)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
